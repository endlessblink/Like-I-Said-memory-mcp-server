{
  "dxt_version": "0.1",
  "name": "like-i-said-memory-v2",
  "display_name": "Like-I-Said Memory Server",
  "version": "2.4.0",
  "description": "Persistent memory and task management for AI assistants with 23 powerful tools",
  "author": {
    "name": "EndlessBlink",
    "email": "support@like-i-said.dev"
  },
  "homepage": "https://github.com/endlessblink/Like-I-Said-memory-mcp-server",
  "license": "MIT",
  "server": {
    "type": "node",
    "entry_point": "server/mcp-server-dxt-optimized.js",
    "mcp_config": {
      "command": "node",
      "args": [
        "${__dirname}/server/mcp-server-dxt-optimized.js"
      ],
      "env": {
        "NODE_ENV": "production"
      }
    }
  },
  "tools": [
    {
      "name": "add_memory",
      "description": "Store information with auto-categorization and linking"
    },
    {
      "name": "get_memory",
      "description": "Retrieve specific memory by ID"
    },
    {
      "name": "list_memories",
      "description": "List memories with filtering and metadata"
    },
    {
      "name": "search_memories",
      "description": "Full-text semantic and keyword search"
    },
    {
      "name": "delete_memory",
      "description": "Remove specific memory permanently"
    },
    {
      "name": "create_task",
      "description": "Create tasks with auto-memory linking"
    },
    {
      "name": "update_task",
      "description": "Update task status and relationships"
    },
    {
      "name": "list_tasks",
      "description": "List tasks with comprehensive filtering"
    },
    {
      "name": "get_task_context",
      "description": "Get full task context with connections"
    },
    {
      "name": "delete_task",
      "description": "Delete tasks and all subtasks"
    },
    {
      "name": "enhance_memory_metadata",
      "description": "Generate optimized titles and summaries"
    },
    {
      "name": "batch_enhance_memories",
      "description": "Batch process memory enhancements"
    },
    {
      "name": "enhance_memory_ollama",
      "description": "Enhance with local AI (Ollama)"
    },
    {
      "name": "batch_enhance_memories_ollama",
      "description": "Batch enhance with local AI"
    },
    {
      "name": "batch_enhance_tasks_ollama",
      "description": "Batch enhance tasks with local AI"
    },
    {
      "name": "smart_status_update",
      "description": "Natural language task status updates"
    },
    {
      "name": "get_task_status_analytics",
      "description": "Comprehensive productivity analytics"
    },
    {
      "name": "validate_task_workflow",
      "description": "Intelligent workflow validation"
    },
    {
      "name": "get_automation_suggestions",
      "description": "AI-powered automation suggestions"
    },
    {
      "name": "generate_dropoff",
      "description": "Generate session handoff documents"
    },
    {
      "name": "deduplicate_memories",
      "description": "Clean up duplicate memory files"
    },
    {
      "name": "check_ollama_status",
      "description": "Check local AI server status"
    },
    {
      "name": "test_tool",
      "description": "Verify MCP connection is working"
    }
  ],
  "user_config": {
    "memory_directory": {
      "type": "string",
      "default": "~/Documents/claude-memories",
      "description": "Directory for storing memory files",
      "required": false
    },
    "task_directory": {
      "type": "string",
      "default": "~/Documents/claude-tasks",
      "description": "Directory for storing task files",
      "required": false
    },
    "default_project": {
      "type": "string",
      "default": "my-project",
      "description": "Default project name for memories and tasks",
      "required": false
    },
    "enable_auto_linking": {
      "type": "boolean",
      "default": true,
      "description": "Automatically link related memories and tasks",
      "required": false
    },
    "max_search_results": {
      "type": "number",
      "default": 25,
      "description": "Maximum number of search results to return",
      "required": false
    },
    "enable_ollama": {
      "type": "boolean",
      "default": false,
      "description": "Enable local AI enhancements with Ollama",
      "required": false
    },
    "ollama_model": {
      "type": "string",
      "default": "llama3.1:8b",
      "description": "Ollama model to use for enhancements",
      "required": false
    }
  },
  "requirements": {
    "node": ">=18.0.0",
    "platforms": [
      "win32",
      "darwin",
      "linux"
    ]
  }
}